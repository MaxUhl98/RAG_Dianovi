{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-03T13:59:42.571472Z",
     "start_time": "2024-11-03T13:59:40.691856Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "torch.set_default_device('cuda')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T13:59:42.576303Z",
     "start_time": "2024-11-03T13:59:42.573471Z"
    }
   },
   "cell_type": "code",
   "source": "models = ['facebook/bart-large-mnli', 'alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli', 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli']",
   "id": "f4274d0bf0eca155",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T14:18:10.403008Z",
     "start_time": "2024-11-03T14:11:07.682718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nli_model = AutoModelForSequenceClassification.from_pretrained('alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli')\n",
    "\n",
    "premise = \"\"\"User:\n",
    "Welches Schmerzmittel sollte ich einem Kleinkind mit gebrochenen Knochen geben?\n",
    "\n",
    "Answer:\n",
    "Schmehig\n",
    "\"\"\"\n",
    "hypothesis = 'Das ist eine passende Antwort'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                     truncation_strategy='only_first')\n",
    "logits = nli_model(x)[0]\n",
    "\n",
    "# we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# \"entailment\" (2) as the probability of the label being true \n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]\n",
    "prob_label_is_true"
   ],
   "id": "a798d58e1e5e8562",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0efbffc55d1745f2b00b5b28eeaa4b2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\models\\Huggingface\\hub\\models--alan-turing-institute--mt5-large-finetuned-mnli-xtreme-xnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac43c2402f0949658a08345b6af36a23"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed47e18f9326412d919e583ac4165f11"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc82d83418a6481fb78b8e60978897fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae8921a0a662409c955334235824cf25"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MT5Tokenizer'. \n",
      "The class this function is called from is 'T5TokenizerFast'.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1496\u001B[0m, in \u001B[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001B[1;34m(self, tiktoken_url)\u001B[0m\n\u001B[0;32m   1495\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1496\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtiktoken\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mload\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_tiktoken_bpe\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1636\u001B[0m, in \u001B[0;36mconvert_slow_tokenizer\u001B[1;34m(transformer_tokenizer, from_tiktoken)\u001B[0m\n\u001B[0;32m   1632\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConverting from Tiktoken\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1633\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mTikTokenConverter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1634\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransformer_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1635\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransformer_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madditional_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 1636\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1637\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1533\u001B[0m, in \u001B[0;36mTikTokenConverter.converted\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1532\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconverted\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tokenizer:\n\u001B[1;32m-> 1533\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1534\u001B[0m     tokenizer\u001B[38;5;241m.\u001B[39mpre_tokenizer \u001B[38;5;241m=\u001B[39m pre_tokenizers\u001B[38;5;241m.\u001B[39mSequence(\n\u001B[0;32m   1535\u001B[0m         [\n\u001B[0;32m   1536\u001B[0m             pre_tokenizers\u001B[38;5;241m.\u001B[39mSplit(Regex(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpattern), behavior\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124misolated\u001B[39m\u001B[38;5;124m\"\u001B[39m, invert\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m   1537\u001B[0m             pre_tokenizers\u001B[38;5;241m.\u001B[39mByteLevel(add_prefix_space\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_prefix_space, use_regex\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m   1538\u001B[0m         ]\n\u001B[0;32m   1539\u001B[0m     )\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1526\u001B[0m, in \u001B[0;36mTikTokenConverter.tokenizer\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1525\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenizer\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m-> 1526\u001B[0m     vocab_scores, merges \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_vocab_merges_from_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1527\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1498\u001B[0m, in \u001B[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001B[1;34m(self, tiktoken_url)\u001B[0m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m-> 1498\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`pip install tiktoken`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1500\u001B[0m     )\n\u001B[0;32m   1502\u001B[0m bpe_ranks \u001B[38;5;241m=\u001B[39m load_tiktoken_bpe(tiktoken_url)\n",
      "\u001B[1;31mValueError\u001B[0m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2447\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2446\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2447\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2448\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m import_protobuf_decode_error():\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:119\u001B[0m, in \u001B[0;36mT5TokenizerFast.__init__\u001B[1;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, add_prefix_space, **kwargs)\u001B[0m\n\u001B[0;32m    117\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_slow\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 119\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43munk_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munk_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextra_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43madditional_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_file \u001B[38;5;241m=\u001B[39m vocab_file\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:138\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madditional_special_tokens \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madditional_special_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m, [])\n\u001B[1;32m--> 138\u001B[0m fast_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_slow_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tiktoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    139\u001B[0m slow_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1638\u001B[0m, in \u001B[0;36mconvert_slow_tokenizer\u001B[1;34m(transformer_tokenizer, from_tiktoken)\u001B[0m\n\u001B[0;32m   1637\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m-> 1638\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1639\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConverting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1640\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith a SentencePiece tokenizer.model file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1641\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrently available slow->fast convertors: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(SLOW_TO_FAST_CONVERTERS\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1642\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m nli_model \u001B[38;5;241m=\u001B[39m AutoModelForSequenceClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124malan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43malan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m premise \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mUser:\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124mWelches Schmerzmittel sollte ich einem Kleinkind mit gebrochenen Knochen geben?\u001B[39m\n\u001B[0;32m      6\u001B[0m \n\u001B[0;32m      7\u001B[0m \u001B[38;5;124mAnswer:\u001B[39m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124mSchmehig\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     10\u001B[0m hypothesis \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDas ist eine passende Antwort\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:920\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    916\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    917\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    918\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTokenizer class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtokenizer_class_candidate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not exist or is not currently imported.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    919\u001B[0m         )\n\u001B[1;32m--> 920\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    922\u001B[0m \u001B[38;5;66;03m# Otherwise we have to be creative.\u001B[39;00m\n\u001B[0;32m    923\u001B[0m \u001B[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001B[39;00m\n\u001B[0;32m    924\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, EncoderDecoderConfig):\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2213\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2210\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2211\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2214\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2215\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2216\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2217\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2218\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2221\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2222\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_is_local\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2224\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2225\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2448\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2446\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   2447\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39minit_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minit_kwargs)\n\u001B[1;32m-> 2448\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[43mimport_protobuf_decode_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   2449\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   2450\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2451\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2452\u001B[0m     )\n\u001B[0;32m   2453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\RAG_Aufgabe\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:87\u001B[0m, in \u001B[0;36mimport_protobuf_decode_error\u001B[1;34m(error_message)\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DecodeError\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(PROTOBUF_IMPORT_ERROR\u001B[38;5;241m.\u001B[39mformat(error_message))\n",
      "\u001B[1;31mImportError\u001B[0m: \n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T14:07:15.807043Z",
     "start_time": "2024-11-03T14:07:15.801581Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e3758e7972da144a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9830], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
